{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDB User Reviews Review Scraper\n",
    "## Hung Pham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step, we will need to import all the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #Using panda to create our dataframe\n",
    "# Import Selenium and its sub libraries\n",
    "import selenium \n",
    "from selenium import webdriver\n",
    "# Import BS4\n",
    "import requests #needed to load the page for BS4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we chose Chrome as our main web browser, we will need to download Chrome driver and tell Selenium where to find it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r\"C:\\Users\\ngoch\\Dropbox\\Data science\\chromedriver.exe\" #path to the webdriver file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can set up our `top100` webpage, it will be the only input we need to feed in the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review(url, folder_name):\n",
    "    '''\n",
    "    Get the review from input as url for IMDB movies list.\n",
    "    The function takes 2 input the url of the movies and the name of the folder to store the data\n",
    "    For each folder, the function will grab the review for each movies and store into respective file.\n",
    "    '''\n",
    "\n",
    "\n",
    "    driver = webdriver.Chrome(PATH) #tell selenium to use Chrome and find the webdriver file in this location\n",
    "    driver.get(url) #tell Selenium to open the webpage\n",
    "    driver.implicitly_wait(1) # tell the webdriver to wait for 1 seconds for the page to load\n",
    "\n",
    "    #After the webpage opened, we can extract the title, hyperlink, year of each movies\n",
    "    #Set initial empty list for each element:\n",
    "    title = []\n",
    "    link = []\n",
    "    year = []\n",
    "\n",
    "    #Grab the block of each individual movie\n",
    "    block = driver.find_elements_by_class_name('lister-item')\n",
    "    #Set up for loop to run through all 50 movies in the first page\n",
    "    for i in range(0,50):\n",
    "        try:\n",
    "            #Extracting title\n",
    "            ftitle = block[i].find_element_by_class_name('lister-item-header').text\n",
    "\n",
    "            #The extracted title has extra elements, so we will have to do some cleaning\n",
    "            #Remove the order in front of the title\n",
    "            forder = block[i].find_element_by_class_name('lister-item-index').text\n",
    "            #Extract the year last 6 letter of the title\n",
    "            fyear = ftitle[-6:]\n",
    "            #Drop the order, year and only keep the movie's name\n",
    "            ftitle = ftitle.replace(forder+' ', '')[:-7 ]\n",
    "            #Then extract the link with cleaned title\n",
    "            flink = block[i].find_element_by_link_text(ftitle).get_attribute('href')\n",
    "\n",
    "            #Add item to the respective lists\n",
    "            title.append(ftitle)\n",
    "            year.append(fyear)\n",
    "            link.append(flink)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # After that, we can use BeautifulSoup to extract the user reviews link \n",
    "    #Set an empty list to store user review link\n",
    "    user_review_links = []\n",
    "    for url in link:\n",
    "        url = url\n",
    "        #setup user agent for BS4, except some rare case, it would be the same for most browser \n",
    "        user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "        #Use request.get to load the whole page\n",
    "        response = requests.get(url, headers = user_agent)\n",
    "        #Parse the request object to BS4 to transform it into html structure\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        #Find the link marked by the USER REVIEWS link text.\n",
    "        review_link = 'https://www.imdb.com'+soup.find('a', text = 'USER REVIEWS').get('href')\n",
    "        #Append the newly grabed link into its list\n",
    "        user_review_links.append(review_link)\n",
    "\n",
    "    #Then create the first data frame to summarize our data at this point:\n",
    "    #Create dictionary for data and columns' name\n",
    "    top_50_data = {'Movie_name': title, \n",
    "            'Year': year, \n",
    "            'link': link,\n",
    "            'user_review' : user_review_links,\n",
    "            }\n",
    "    top50 = pd.DataFrame(data = top_50_data) #create dataframe\n",
    "    driver.quit() #tell Selenium to close the webpage\n",
    "\n",
    "    # Step 2, we will grab the data from each user review page\n",
    "    # Use Selenium to go to each user review page\n",
    "    for i in range(len(top50['user_review'])): \n",
    "        driver = webdriver.Chrome(PATH)\n",
    "        driver.get(top50['user_review'][i])\n",
    "        driver.implicitly_wait(1) # tell the webdriver to wait for 1 seconds for the page to load to prevent blocked by anti spam software\n",
    "\n",
    "\n",
    "        # Set up action to click on 'load more' button\n",
    "        # note that each page on imdb has 25 reviews\n",
    "        page = 1 #Set initial variable for while loop\n",
    "        #We want at least 1000 review, so get 50 at a safe number\n",
    "        while page<50:  \n",
    "            try:\n",
    "                #find the load more button on the webpage\n",
    "                load_more = driver.find_element_by_id('load-more-trigger')\n",
    "                #click on that button\n",
    "                load_more.click()\n",
    "                page+=1 #move on to next loadmore button\n",
    "            except:\n",
    "                #If couldnt find any button to click, stop\n",
    "                break\n",
    "        # After fully expand the page, we will grab data from whole website\n",
    "        review = driver.find_elements_by_class_name('review-container')\n",
    "        #Set list for each element:\n",
    "        title = []\n",
    "        content = []\n",
    "        rating = []\n",
    "        date = []\n",
    "        user_name = []\n",
    "        #run for loop to get \n",
    "        for n in range(0,1100):\n",
    "            try:\n",
    "                #Some reviewers only give review text or rating without the other, \n",
    "                #so we use try/except here to make sure each block of content must has all the element before append them to the list\n",
    "\n",
    "                #Check if each review has all the elements\n",
    "                ftitle = review[n].find_element_by_class_name('title').text\n",
    "                #For the review content, some of them are hidden as spoiler, \n",
    "                #so we use the attribute 'textContent' here after extracting the 'content' tag\n",
    "                fcontent = review[n].find_element_by_class_name('content').get_attribute(\"textContent\").strip()\n",
    "                frating = review[n].find_element_by_class_name('rating-other-user-rating').text\n",
    "                fdate = review[n].find_element_by_class_name('review-date').text\n",
    "                fname = review[n].find_element_by_class_name('display-name-link').text\n",
    "\n",
    "\n",
    "                #Then add them to the respective list\n",
    "                title.append(ftitle)\n",
    "                content.append(fcontent)\n",
    "                rating.append(frating)\n",
    "                date.append(fdate)\n",
    "                user_name.append(fname)\n",
    "            except:\n",
    "                continue\n",
    "        #Build data dictionary for dataframe\n",
    "        data = {'User_name': user_name, \n",
    "            'Review title': title, \n",
    "            'Review Rating': rating,\n",
    "            'Review date' : date,\n",
    "            'Review_body' : content\n",
    "           }\n",
    "        #Build dataframe for each movie to export\n",
    "        review = pd.DataFrame(data = data)\n",
    "        movie = top50['Movie_name'][i] #grab the movie name from the top50 list    \n",
    "        review['Movie_name'] = movie #create new column with the same movie name column    \n",
    "        review.to_csv(f'data/{folder_name}/{i+1}.csv') #store them into individual file for each movies, so we can combine or check them later\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can save the newly created raw dataframe into a csv file for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole process gave us around 50k reviews for top 50 highest rating movies on IMDB, but as these are the best of the best, I expected that the rating will be heavily imblanced as most of them will be 9/10 score. To combat that, I also repeat the process, but with top 50 lowest rating movies. \n",
    "So the raw data after combined will have reviews from both good and bad movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top50_raw_link = 'https://www.imdb.com/search/title/?groups=top_100'\n",
    "get_review(top50_raw_link, 'top50_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 cells below are the code to grab the data and save them into respective folder inside main `Data` folder.  \n",
    "I wont pre-run these cells again as all they do is write the `csv` file and give no output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot50_2_link = 'https://www.imdb.com/search/title/?groups=bottom_100&start=51&ref_=adv_nxt'\n",
    "get_review(bot50_2_link, 'bottom50_2_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot50_1_link = 'https://www.imdb.com/search/title/?groups=bottom_100&ref_=adv_prv'\n",
    "get_review(bot50_1_link, 'bottom50_1_raw')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
