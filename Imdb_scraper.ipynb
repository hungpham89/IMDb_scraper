{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: IMDB User Reviews Rating predictor\n",
    "## Hung Pham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction and Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is one of the earliest problem come to mind when dealing with NLP. With the world moving to a new digital age with almost anything can be bought and sold on the internet, knowing if an review is positive or negative toward the product is crucial for any business owner. These review may contains within it the hint to improve a product or avoid the drawback.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common practical application for sentiment analysis is with review. Normally, the data analyst practitioner will use the rating score to separate all the review into good and bad review, and just perform the analysis with them. The threshold to decide good from bad review is differ case by case, but most of the time, it would be enough to learn what make a product good or bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we want to make it further, say if we have a review and instead of knowing if the review is positive or negative, we want to actually predict the score that reviewer would give to this product. That isn't an easy task, even for human. It's even harder than a normal multi classes classification, as the classes here is closely linked with each other in a hierarchy order, instead of completely distinguishable classes (think about trying to predict if it is a 5 or 6 score versus predict if it is a cat or a dog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the tools that we have in our arsenal, I decided to push it further and see how far we can go in this challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Data collecting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a movies fan, I wanted to use IMDB users review database with this project. To my surprise, there aren't any dataset for it available in the popular data set collection website (Kaggle, Google analytic...).   \n",
    "So I decided to scrap the data directly from IDMB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of IMDB is rather straight forward, each movies will have its own page contains all the information of the movies. Our main interest is the user review page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the `USER REVIEWS` link, we will be directed to the corresponding page of the movie's user reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After review functionalities and limit of several data scraping tools, I decided to use a combination of **Selenium** and **BeautifulSoup** due to their flexibility and beginner friendliness. The drawback is that both are quite slow.  \n",
    "The detail step by step process should look like this:  \n",
    "- The only input we need is the `Top100 movies` website, we will use Selenium to grab all the hyperlink of all the individual movies and store them into `movies link` list.  \n",
    "- Use BS4 to go to each movie website to grab it user review link, store them into `user_review_links` list.\n",
    "- Go to each link in `user_review_links`, as IMDB's USER REVIEW pages are defaulted to show only 25 reviews each page and we will have to click on `load more` button to expand it. We can use Selenium to do this task for us. \n",
    "- After expanding the page, we can extract the reviews into their respected lists: `Rating`, `User_name`, `Review_text`, `Review_title`, `Review_date`. We can then create a dataframe for each movie and out them into a csv file for later use.  \n",
    "- Combine all the individual csv files back into our final dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Planning is done, let's get started!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. Scraping data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step, we will need to import all the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #Using panda to create our dataframe\n",
    "# Import Selenium and its sub libraries\n",
    "import selenium \n",
    "from selenium import webdriver\n",
    "# Import BS4\n",
    "import requests #needed to load the page for BS4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we chose Chrome as our main web browser, we will need to download Chrome driver and tell Selenium where to find it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r\"C:\\Users\\ngoch\\Dropbox\\Data science\\chromedriver.exe\" #path to the webdriver file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can set up our `top100` webpage, it will be the only input we need to feed in the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review(url, folder_name):\n",
    "    '''\n",
    "    Get the review from input as url for IMDB movies list.\n",
    "    The function takes 2 input the url of the movies and the name of the folder to store the data\n",
    "    For each folder, the function will grab the review for each movies and store into respective file.\n",
    "    '''\n",
    "\n",
    "\n",
    "    driver = webdriver.Chrome(PATH) #tell selenium to use Chrome and find the webdriver file in this location\n",
    "    driver.get(url) #tell Selenium to open the webpage\n",
    "    driver.implicitly_wait(1) # tell the webdriver to wait for 1 seconds for the page to load\n",
    "\n",
    "    #After the webpage opened, we can extract the title, hyperlink, year of each movies\n",
    "    #Set initial empty list for each element:\n",
    "    title = []\n",
    "    link = []\n",
    "    year = []\n",
    "\n",
    "    #Grab the block of each individual movie\n",
    "    block = driver.find_elements_by_class_name('lister-item')\n",
    "    #Set up for loop to run through all 50 movies in the first page\n",
    "    for i in range(0,50):\n",
    "        try:\n",
    "            #Extracting title\n",
    "            ftitle = block[i].find_element_by_class_name('lister-item-header').text\n",
    "\n",
    "            #The extracted title has extra elements, so we will have to do some cleaning\n",
    "            #Remove the order in front of the title\n",
    "            forder = block[i].find_element_by_class_name('lister-item-index').text\n",
    "            #Extract the year last 6 letter of the title\n",
    "            fyear = ftitle[-6:]\n",
    "            #Drop the order, year and only keep the movie's name\n",
    "            ftitle = ftitle.replace(forder+' ', '')[:-7 ]\n",
    "            #Then extract the link with cleaned title\n",
    "            flink = block[i].find_element_by_link_text(ftitle).get_attribute('href')\n",
    "\n",
    "            #Add item to the respective lists\n",
    "            title.append(ftitle)\n",
    "            year.append(fyear)\n",
    "            link.append(flink)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # After that, we can use BeautifulSoup to extract the user reviews link \n",
    "    #Set an empty list to store user review link\n",
    "    user_review_links = []\n",
    "    for url in link:\n",
    "        url = url\n",
    "        #setup user agent for BS4, except some rare case, it would be the same for most browser \n",
    "        user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "        #Use request.get to load the whole page\n",
    "        response = requests.get(url, headers = user_agent)\n",
    "        #Parse the request object to BS4 to transform it into html structure\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        #Find the link marked by the USER REVIEWS link text.\n",
    "        review_link = 'https://www.imdb.com'+soup.find('a', text = 'USER REVIEWS').get('href')\n",
    "        #Append the newly grabed link into its list\n",
    "        user_review_links.append(review_link)\n",
    "\n",
    "    #Then create the first data frame to summarize our data at this point:\n",
    "    #Create dictionary for data and columns' name\n",
    "    top_50_data = {'Movie_name': title, \n",
    "            'Year': year, \n",
    "            'link': link,\n",
    "            'user_review' : user_review_links,\n",
    "            }\n",
    "    top50 = pd.DataFrame(data = top_50_data) #create dataframe\n",
    "    driver.quit() #tell Selenium to close the webpage\n",
    "\n",
    "    # Step 2, we will grab the data from each user review page\n",
    "    # Use Selenium to go to each user review page\n",
    "    for i in range(len(top50['user_review'])): \n",
    "        driver = webdriver.Chrome(PATH)\n",
    "        driver.get(top50['user_review'][i])\n",
    "        driver.implicitly_wait(1) # tell the webdriver to wait for 1 seconds for the page to load to prevent blocked by anti spam software\n",
    "\n",
    "\n",
    "        # Set up action to click on 'load more' button\n",
    "        # note that each page on imdb has 25 reviews\n",
    "        page = 1 #Set initial variable for while loop\n",
    "        #We want at least 1000 review, so get 50 at a safe number\n",
    "        while page<50:  \n",
    "            try:\n",
    "                #find the load more button on the webpage\n",
    "                load_more = driver.find_element_by_id('load-more-trigger')\n",
    "                #click on that button\n",
    "                load_more.click()\n",
    "                page+=1 #move on to next loadmore button\n",
    "            except:\n",
    "                #If couldnt find any button to click, stop\n",
    "                break\n",
    "        # After fully expand the page, we will grab data from whole website\n",
    "        review = driver.find_elements_by_class_name('review-container')\n",
    "        #Set list for each element:\n",
    "        title = []\n",
    "        content = []\n",
    "        rating = []\n",
    "        date = []\n",
    "        user_name = []\n",
    "        #run for loop to get \n",
    "        for n in range(0,1100):\n",
    "            try:\n",
    "                #Some reviewers only give review text or rating without the other, \n",
    "                #so we use try/except here to make sure each block of content must has all the element before append them to the list\n",
    "\n",
    "                #Check if each review has all the elements\n",
    "                ftitle = review[n].find_element_by_class_name('title').text\n",
    "                #For the review content, some of them are hidden as spoiler, \n",
    "                #so we use the attribute 'textContent' here after extracting the 'content' tag\n",
    "                fcontent = review[n].find_element_by_class_name('content').get_attribute(\"textContent\").strip()\n",
    "                frating = review[n].find_element_by_class_name('rating-other-user-rating').text\n",
    "                fdate = review[n].find_element_by_class_name('review-date').text\n",
    "                fname = review[n].find_element_by_class_name('display-name-link').text\n",
    "\n",
    "\n",
    "                #Then add them to the respective list\n",
    "                title.append(ftitle)\n",
    "                content.append(fcontent)\n",
    "                rating.append(frating)\n",
    "                date.append(fdate)\n",
    "                user_name.append(fname)\n",
    "            except:\n",
    "                continue\n",
    "        #Build data dictionary for dataframe\n",
    "        data = {'User_name': user_name, \n",
    "            'Review title': title, \n",
    "            'Review Rating': rating,\n",
    "            'Review date' : date,\n",
    "            'Review_body' : content\n",
    "           }\n",
    "        #Build dataframe for each movie to export\n",
    "        review = pd.DataFrame(data = data)\n",
    "        movie = top50['Movie_name'][i] #grab the movie name from the top50 list    \n",
    "        review['Movie_name'] = movie #create new column with the same movie name column    \n",
    "        review.to_csv(f'data/{folder_name}/{i+1}.csv') #store them into individual file for each movies, so we can combine or check them later\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can save the newly created raw dataframe into a csv file for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole process gave us around 50k reviews for top 50 highest rating movies on IMDB, but as these are the best of the best, I expected that the rating will be heavily imblanced as most of them will be 9/10 score. To combat that, I also repeat the process, but with top 50 lowest rating movies. \n",
    "So the raw data after combined will have reviews from both good and bad movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 cells below are the code to grab the data and save them into respective folder inside main `Data` folder.  \n",
    "I wont pre-run these cells again as all they do is write the `csv` file and give no output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top50_raw_link = 'https://www.imdb.com/search/title/?groups=top_100'\n",
    "get_review(top50_raw_link, 'top50_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot50_2_link = 'https://www.imdb.com/search/title/?groups=bottom_100&start=51&ref_=adv_nxt'\n",
    "get_review(bot50_2_link, 'bottom50_2_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot50_1_link = 'https://www.imdb.com/search/title/?groups=bottom_100&ref_=adv_prv'\n",
    "get_review(bot50_1_link, 'bottom50_1_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting our data. Let's combine them together and have some EDA in Part2_Cleaning_and_EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_raw.to_csv('data/bad_review_half.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_raw1 = df_bad_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_raw2 = pd.read_csv('data/review_bad1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_raw = pd.concat([df_bad_raw1,df_bad_raw2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_raw.drop(['Unnamed: 0', 'Unnamed: 0.1'],axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_raw['Review_body'] = df_bad_raw['Review_body'].str.replace('\\n', '') # Remove linebreak symbol\n",
    "df_bad_raw['Review_body'] = df_bad_raw['Review_body'].str.replace('Was this review helpful?', '')\n",
    "df_bad_raw['Review_body'] = df_bad_raw['Review_body'].str.replace('Sign in to vote. ', '')\n",
    "df_bad_raw['Review_body'] = df_bad_raw['Review_body'].str.replace('Permalink', '')\n",
    "df_bad_raw['Review_body'] = [re.sub(r\"\\s{2,}\", \"\", x) for x in df_bad_raw['Review_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_raw.columns = ['User_name', 'Review_title', 'Review_Rating', 'Review_date',\n",
    "       'Review_body', 'Movie_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_raw[['Review_Rating','temp']] = df_bad_raw['Review_Rating'].str.split(pat = '/', expand = True)\n",
    "df_bad_raw.drop('temp', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_raw['Review_date'] = pd.to_datetime(df_bad_raw['Review_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_raw['Review_Rating'] = df_bad_raw['Review_Rating'].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_raw['year'] = df_bad_raw['Review_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_raw['month'] = df_bad_raw['Review_date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_raw['weekday'] = df_bad_raw['Review_date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_bad_rawf = df_bad_raw.drop('Review_date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_raw.to_csv('data/review_bad_cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df_bad_raw['Review_Rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
